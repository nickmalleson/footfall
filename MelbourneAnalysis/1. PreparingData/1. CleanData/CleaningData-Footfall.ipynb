{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Melbourne sensor footfall data \n",
    "This script cleans the footfall data for Melbourne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T09:29:55.509799Z",
     "start_time": "2020-05-20T09:29:52.577608Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import date, datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import pedestrian count data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_counts = pd.read_csv('../../Data/FootfallData/Pedestrian_Counting_System_Monthly_counts_per_hour_may_2009_to_14_dec_2022.csv')\n",
    "sensor_locations = pd.read_csv('../../Data/FootfallData/melbourne_locations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns to all lowercase (to facilitate joining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_counts.rename({'Date_Time': 'datetime', 'Year': 'year', 'Month':'month', 'Mdate': 'mdate', \n",
    "                      'Day': 'day', 'Time': 'time', 'Sensor_ID': 'sensor_id', 'Hourly_Counts': 'hourly_counts'}, \n",
    "                     axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_counts.drop(['ID', 'Sensor_Name'], axis = 1, inplace = True)\n",
    "sensor_locations.drop(['sensor_description', 'sensor_name', 'installation_date', 'status', 'note', 'direction_1',\n",
    "                      'direction_2'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join two dataframes so location and count info in same place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_counts = pd.merge(sensor_locations, sensor_counts, on='sensor_id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It could be useful to drop sensors that do not have as many recorded countsfor now leaving them in, but may drop in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Properly format datetime column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_counts['datetime'] = pd.to_datetime(location_counts['datetime'], format = '%B %d, %Y %I:%M:%S %p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Order by datetime column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_counts = location_counts.sort_values(by=['datetime'])\n",
    "location_counts.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### September, 2010 dates have problems:\n",
    "All dates have a timestamp of 00:00, presume they are in order of hour of day, for each day there are only 23 hours worth of data\n",
    "For now, in later stages just filter out 2010 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_counts = location_counts[location_counts['year']>2010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location_counts.drop(['Latitude', 'Longitude', 'location'], axis = 1, inplace =True)\n",
    "# location_counts.rename({'day': 'weekday', 'mdate': 'day', 'time': 'hour'}, axis =1, inplace = True)\n",
    "\n",
    "# Change month to integers 1-12\n",
    "months = {'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June':6, 'July': 7, 'August': 8,\n",
    "         'September': 9, 'October': 10, 'November': 11, 'December': 12}\n",
    "\n",
    "location_counts.month = location_counts.month.map(months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_some_sensors = location_counts[location_counts['sensor_id'].isin([2,6,8,9,10,11,18])]\n",
    "just_some_sensors.to_csv(\"../../Cleaned_data/validsensors.csv\",index=False)\n",
    "location_counts.to_csv(\"../../Cleaned_data/allsensors.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get data for just one sensor\n",
    "# one_sensor =  location_counts[location_counts.sensor_id == 4]\n",
    "# one_year = one_sensor[one_sensor.year==2010]\n",
    "# # Set the datetime as the index\n",
    "# one_sensor.set_index('datetime', inplace = True)\n",
    "# duplicates = one_sensor[one_sensor.index.duplicated()]\n",
    "# for day in duplicates.mdate.unique():\n",
    "#     one_day = duplicates[duplicates.mdate == day]\n",
    "#     print(day, len(one_day))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubleMADsfromMedian(y,thresh=3.5):\n",
    "    \"\"\"Find outliers using the Median Average Distance.\n",
    "    \n",
    "    VALUE: return a list of true/false denoting whether the element in y is an outlier or not\n",
    "    \n",
    "    PARAMETERS:\n",
    "      - y is a pandas Series, or something like that.\n",
    "    \n",
    "    warning: this function does not check for NAs\n",
    "    nor does it address issues when \n",
    "    more than 50% of your data have identical values\n",
    "    \"\"\"\n",
    "    # Calculate the upper and lower limits\n",
    "    m = np.median(y) # The median\n",
    "    abs_dev = np.abs(y - m) # The absolute difference between each y and the median\n",
    "    # The upper and lower limits are the median of the difference\n",
    "    # of each data point from the median of the data\n",
    "    left_mad = np.median(abs_dev[y <= m]) # The left limit (median of lower half)\n",
    "    right_mad = np.median(abs_dev[y >= m]) # The right limit (median of upper half)\n",
    "    \n",
    "    # Now create an array where each value has left_mad if it is in the lower half of the data,\n",
    "    # or right_mad if it is in the upper half\n",
    "    y_mad = left_mad * np.ones(len(y)) # Initially every value is 'left_mad'\n",
    "    y_mad[y > m] = right_mad # Now larger values are right_mad\n",
    "\n",
    "    # Calculate the z scores for each element\n",
    "    modified_z_score = 0.6745 * abs_dev / y_mad\n",
    "    modified_z_score[y == m] = 0\n",
    "    \n",
    "    # Return boolean list showing whether each y is an outlier\n",
    "    return modified_z_score > thresh\n",
    "\n",
    "# Make a list of true/false for whether the footfall is an outlier\n",
    "no_outliers = pd.DataFrame(doubleMADsfromMedian(original['InCount']))\n",
    "no_outliers.columns = ['outlier'] # Rename the column to 'outlier'\n",
    "\n",
    "# Join to the original footfall data to the list of outliers, then select a few useful columns\n",
    "join = pd.concat([original, no_outliers], axis = 1)\n",
    "join = pd.DataFrame(join, columns = ['Day_yr', 'outlier', 'InCount'])\n",
    "\n",
    "# Choose just the outliers\n",
    "outliers = join[join['outlier'] == True]\n",
    "outliers_list = list(outliers['Day_yr']) # A list of the days that are outliers\n",
    "\n",
    "# Now remove all outliers from the original data\n",
    "df = original.loc[~original['Day_yr'].isin(outliers_list)]\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "# Check that the lengths all make sense\n",
    "assert(len(df) == len(original)-len(outliers_list))\n",
    "\n",
    "print(\"I found {} outliers from {} days in total. Removing them leaves us with {} events\".format(\\\n",
    "    len(outliers_list), len(join), len(df) ) )"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
