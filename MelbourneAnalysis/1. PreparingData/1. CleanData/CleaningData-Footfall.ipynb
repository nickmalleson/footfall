{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Melbourne sensor footfall data \n",
    "This script cleans the footfall data for Melbourne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code initialisation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T17:02:24.732989Z",
     "start_time": "2024-09-05T17:02:24.730200Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from datetime import date, datetime\n",
    "\n",
    "from tests.conftest import data_dir\n",
    "\n",
    "# Add the path two directories up from the current notebook location\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "# Now import the function from DataPrepFunctions.py\n",
    "from DataPrepFunctions import check_and_prepare_data_directory"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import pedestrian count data\n",
    "\n",
    "TODO: Need to upload the data to a permanent location and check that the download/extract function works."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T17:09:39.844993Z",
     "start_time": "2024-09-05T17:09:37.667565Z"
    }
   },
   "source": [
    "check_and_prepare_data_directory(data_dir=\"../../Data/\", \n",
    "                                 file_url=\"www.my_domain/my_data.zip\",)\n",
    "sensor_counts = pd.read_csv('../../Data/FootfallData/Pedestrian_Counting_System_Monthly_counts_per_hour_may_2009_to_14_dec_2022.csv')\n",
    "sensor_locations = pd.read_csv('../../Data/FootfallData/melbourne_locations.csv')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "Data directory '../../Data/' already contains files.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns to all lowercase (to facilitate joining)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T17:09:39.850399Z",
     "start_time": "2024-09-05T17:09:39.848372Z"
    }
   },
   "source": [
    "sensor_counts.rename({'Date_Time': 'datetime', 'Year': 'year', 'Month':'month', 'Mdate': 'mdate', \n",
    "                      'Day': 'day', 'Time': 'time', 'Sensor_ID': 'sensor_id', 'Hourly_Counts': 'hourly_counts'}, \n",
    "                     axis = 1, inplace = True)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T17:09:39.975663Z",
     "start_time": "2024-09-05T17:09:39.859699Z"
    }
   },
   "source": [
    "sensor_counts.drop(['ID', 'Sensor_Name'], axis = 1, inplace = True)\n",
    "#sensor_locations.drop(['sensor_description', 'sensor_name', 'installation_date', 'status', 'note', 'direction_1',\n",
    "#                      'direction_2'], axis = 1, inplace = True)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join two dataframes so location and count info in same place"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T17:09:45.990295Z",
     "start_time": "2024-09-05T17:09:45.056803Z"
    }
   },
   "source": [
    "location_counts = pd.merge(sensor_locations, sensor_counts, on='sensor_id', how='inner')"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It could be useful to drop sensors that do not have as many recorded countsfor now leaving them in, but may drop in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Properly format datetime column"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T17:10:02.940999Z",
     "start_time": "2024-09-05T17:09:53.887544Z"
    }
   },
   "source": [
    "location_counts['datetime'] = pd.to_datetime(location_counts['datetime'], format = '%B %d, %Y %I:%M:%S %p')"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Order by datetime column"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T17:10:22.081995Z",
     "start_time": "2024-09-05T17:10:21.519573Z"
    }
   },
   "source": [
    "location_counts = location_counts.sort_values(by=['datetime'])\n",
    "location_counts.reset_index(inplace = True, drop = True)"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### September, 2010 dates have problems:\n",
    "All dates have a timestamp of 00:00, presume they are in order of hour of day, for each day there are only 23 hours worth of data\n",
    "For now, in later stages just filter out 2010 data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T17:10:28.015537Z",
     "start_time": "2024-09-05T17:10:27.693443Z"
    }
   },
   "source": [
    "location_counts = location_counts[location_counts['year']>2010]"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and save data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T17:10:32.165323Z",
     "start_time": "2024-09-05T17:10:32.001442Z"
    }
   },
   "source": [
    "# location_counts.drop(['Latitude', 'Longitude', 'location'], axis = 1, inplace =True)\n",
    "# location_counts.rename({'day': 'weekday', 'mdate': 'day', 'time': 'hour'}, axis =1, inplace = True)\n",
    "\n",
    "# Change month to integers 1-12\n",
    "months = {'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June':6, 'July': 7, 'August': 8,\n",
    "         'September': 9, 'October': 10, 'November': 11, 'December': 12}\n",
    "\n",
    "location_counts.month = location_counts.month.map(months)"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T17:11:07.720365Z",
     "start_time": "2024-09-05T17:10:42.270206Z"
    }
   },
   "source": [
    "just_some_sensors = location_counts[location_counts['sensor_id'].isin([2,6,8,9,10,11,18])]\n",
    "just_some_sensors.to_csv(\"../../Cleaned_data/validsensors.csv\",index=False)\n",
    "location_counts.to_csv(\"../../Cleaned_data/allsensors.csv\",index=False)"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get data for just one sensor\n",
    "# one_sensor =  location_counts[location_counts.sensor_id == 4]\n",
    "# one_year = one_sensor[one_sensor.year==2010]\n",
    "# # Set the datetime as the index\n",
    "# one_sensor.set_index('datetime', inplace = True)\n",
    "# duplicates = one_sensor[one_sensor.index.duplicated()]\n",
    "# for day in duplicates.mdate.unique():\n",
    "#     one_day = duplicates[duplicates.mdate == day]\n",
    "#     print(day, len(one_day))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier removal\n",
    "\n",
    "(_I think this is redundant now, performed before modelling_)\n",
    "\n",
    "TODO: Remove following this redundant outlier removal code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubleMADsfromMedian(y,thresh=3.5):\n",
    "    \"\"\"Find outliers using the Median Average Distance.\n",
    "    \n",
    "    VALUE: return a list of true/false denoting whether the element in y is an outlier or not\n",
    "    \n",
    "    PARAMETERS:\n",
    "      - y is a pandas Series, or something like that.\n",
    "    \n",
    "    warning: this function does not check for NAs\n",
    "    nor does it address issues when \n",
    "    more than 50% of your data have identical values\n",
    "    \"\"\"\n",
    "    # Calculate the upper and lower limits\n",
    "    m = np.median(y) # The median\n",
    "    abs_dev = np.abs(y - m) # The absolute difference between each y and the median\n",
    "    # The upper and lower limits are the median of the difference\n",
    "    # of each data point from the median of the data\n",
    "    left_mad = np.median(abs_dev[y <= m]) # The left limit (median of lower half)\n",
    "    right_mad = np.median(abs_dev[y >= m]) # The right limit (median of upper half)\n",
    "    \n",
    "    # Now create an array where each value has left_mad if it is in the lower half of the data,\n",
    "    # or right_mad if it is in the upper half\n",
    "    y_mad = left_mad * np.ones(len(y)) # Initially every value is 'left_mad'\n",
    "    y_mad[y > m] = right_mad # Now larger values are right_mad\n",
    "\n",
    "    # Calculate the z scores for each element\n",
    "    modified_z_score = 0.6745 * abs_dev / y_mad\n",
    "    modified_z_score[y == m] = 0\n",
    "    \n",
    "    # Return boolean list showing whether each y is an outlier\n",
    "    return modified_z_score > thresh\n",
    "\n",
    "# Make a list of true/false for whether the footfall is an outlier\n",
    "no_outliers = pd.DataFrame(doubleMADsfromMedian(original['InCount']))\n",
    "no_outliers.columns = ['outlier'] # Rename the column to 'outlier'\n",
    "\n",
    "# Join to the original footfall data to the list of outliers, then select a few useful columns\n",
    "join = pd.concat([original, no_outliers], axis = 1)\n",
    "join = pd.DataFrame(join, columns = ['Day_yr', 'outlier', 'InCount'])\n",
    "\n",
    "# Choose just the outliers\n",
    "outliers = join[join['outlier'] == True]\n",
    "outliers_list = list(outliers['Day_yr']) # A list of the days that are outliers\n",
    "\n",
    "# Now remove all outliers from the original data\n",
    "df = original.loc[~original['Day_yr'].isin(outliers_list)]\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "# Check that the lengths all make sense\n",
    "assert(len(df) == len(original)-len(outliers_list))\n",
    "\n",
    "print(\"I found {} outliers from {} days in total. Removing them leaves us with {} events\".format(\\\n",
    "    len(outliers_list), len(join), len(df) ) )"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
